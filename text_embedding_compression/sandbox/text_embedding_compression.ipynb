{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(len(tokenizer))  # Output: 30522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT tokenization:\n",
      "Tokens: [101, 6240, 1846, 6165, 1110, 19601, 106, 102]\n",
      "Words: ['[CLS]', 'meat', '後', 'rates', 'ɑ', 'facilitated', '[unused101]', '[SEP]']\n",
      "\n",
      "FastText tokenization:\n",
      "Tokens (words): ['Natural', 'language', 'processing', 'is', 'fascinating!']\n",
      "\n",
      "OpenAI tokenization:\n",
      "Tokens: [55381, 4221, 8863, 374, 27387, 0]\n",
      "Words: [b'Natural', b' language', b' processing', b' is', b' fascinating', b'!']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, T5Tokenizer\n",
    "import tiktoken\n",
    "\n",
    "def tokenize_bert(text):\n",
    "    \"\"\"Tokenize text using BERT tokenizer\"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_t5(text):\n",
    "    \"\"\"Tokenize text using T5 tokenizer\"\"\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_fasttext(text):\n",
    "    \"\"\"Tokenize text using FastText\"\"\"\n",
    "    # FastText uses simple space-based tokenization\n",
    "    return text.split()\n",
    "\n",
    "def tokenize_openai(text, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Tokenize text using OpenAI's tiktoken\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "text = \"Natural language processing is fascinating!\"\n",
    "\n",
    "print(\"BERT tokenization:\")\n",
    "bert_tokens = tokenize_bert(text)\n",
    "print(\"Tokens:\", bert_tokens)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(\"Words:\", tokenizer.convert_ids_to_tokens(bert_tokens))\n",
    "\n",
    "# print(\"\\nT5 tokenization:\")\n",
    "# t5_tokens = tokenize_t5(text)\n",
    "# print(\"Tokens:\", t5_tokens)\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "# print(\"Words:\", tokenizer.convert_ids_to_tokens(t5_tokens))\n",
    "\n",
    "print(\"\\nFastText tokenization:\")\n",
    "fasttext_tokens = tokenize_fasttext(text)\n",
    "print(\"Tokens (words):\", fasttext_tokens)\n",
    "\n",
    "print(\"\\nOpenAI tokenization:\") \n",
    "openai_tokens = tokenize_openai(text)\n",
    "print(\"Tokens:\", openai_tokens)\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "print(\"Words:\", encoding.decode_tokens_bytes(openai_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_tokenization(text):\n",
    "    print('text length (bytes):', len(text))\n",
    "\n",
    "    words = text.split()\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    print(f\"\\nAverage word length: {avg_word_length:.2f} characters ) add one for space\\n\")\n",
    "\n",
    "    print(\"BERT tokenization:\")\n",
    "    bert_tokens = tokenize_bert(text)\n",
    "    print(\"Tokens:\", bert_tokens)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    words_list_bert = tokenizer.convert_ids_to_tokens(bert_tokens)\n",
    "    print(\"Words:\", words_list_bert)\n",
    "    print(f'{len(bert_tokens)} tokens, about {2*len(bert_tokens)} bytes')\n",
    "\n",
    "    print(\"\\nOpenAI tokenization:\") \n",
    "    openai_tokens = tokenize_openai(text)\n",
    "    print(\"Tokens:\", openai_tokens)\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    words_list_openai = encoding.decode_tokens_bytes(openai_tokens)\n",
    "    print(\"Words:\", words_list_openai)\n",
    "    print(f'{len(openai_tokens)} tokens, about {3*len(openai_tokens)} bytes')\n",
    "\n",
    "\n",
    "    return words_list_bert, words_list_openai\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length (bytes): 37 \n",
      "\n",
      "BERT tokenization:\n",
      "Tokens: [101, 2202, 2033, 2000, 1996, 2190, 2717, 18842, 2102, 1999, 2237, 102]\n",
      "Words: ['[CLS]', 'take', 'me', 'to', 'the', 'best', 'rest', '##oran', '##t', 'in', 'town', '[SEP]']\n",
      "12 tokens, about 24 bytes\n",
      "\n",
      "FastText tokenization:\n",
      "Tokens (words): ['take', 'me', 'to', 'the', 'best', 'restorant', 'in', 'town']\n",
      "8\n",
      "\n",
      "OpenAI tokenization:\n",
      "Tokens: [23609, 757, 311, 279, 1888, 2800, 269, 519, 304, 6424]\n",
      "Words: [b'take', b' me', b' to', b' the', b' best', b' rest', b'or', b'ant', b' in', b' town']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"take me to the best restorant in town\"\n",
    "example_tokenization(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length (bytes): 33 \n",
      "\n",
      "BERT tokenization:\n",
      "Tokens: [101, 3565, 9289, 10128, 14220, 29282, 4588, 10288, 19312, 21273, 17369, 2229, 102]\n",
      "Words: ['[CLS]', 'super', '##cal', '##if', '##raj', '##elis', '##tic', '##ex', '##pia', '##lid', '##osh', '##es', '[SEP]']\n",
      "13 tokens, about 26 bytes\n",
      "\n",
      "FastText tokenization:\n",
      "Tokens (words): ['Supercalifrajelisticexpialidoshes']\n",
      "1\n",
      "\n",
      "OpenAI tokenization:\n",
      "Tokens: [10254, 3035, 278, 333, 56486, 301, 4633, 4683, 532, 307, 9451, 288]\n",
      "Words: [b'Sup', b'erc', b'al', b'if', b'raj', b'el', b'istic', b'exp', b'ial', b'id', b'osh', b'es']\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Supercalifrajelisticexpialidoshes\"\n",
    "example_tokenization(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length (bytes): 122 \n",
      "\n",
      "BERT tokenization:\n",
      "Tokens: [101, 2122, 4275, 2079, 2025, 2224, 1037, 4964, 2773, 16188, 1010, 2021, 2738, 1037, 19204, 16188, 1006, 2241, 2006, 4942, 22104, 2478, 17531, 2063, 2030, 2773, 11198, 1007, 1012, 102]\n",
      "Words: ['[CLS]', 'these', 'models', 'do', 'not', 'use', 'a', 'fixed', 'word', 'vocabulary', ',', 'but', 'rather', 'a', 'token', 'vocabulary', '(', 'based', 'on', 'sub', '##words', 'using', 'bp', '##e', 'or', 'word', '##piece', ')', '.', '[SEP]']\n",
      "30 tokens, about 60 bytes\n",
      "\n",
      "FastText tokenization:\n",
      "Tokens (words): ['These', 'models', 'do', 'not', 'use', 'a', 'fixed', 'word', 'vocabulary,', 'but', 'rather', 'a', 'token', 'vocabulary', '(based', 'on', 'subwords', 'using', 'BPE', 'or', 'WordPiece).']\n",
      "21\n",
      "\n",
      "OpenAI tokenization:\n",
      "Tokens: [9673, 4211, 656, 539, 1005, 264, 8521, 3492, 36018, 11, 719, 4856, 264, 4037, 36018, 320, 31039, 389, 1207, 5880, 1701, 426, 1777, 477, 9506, 32309, 570]\n",
      "Words: [b'These', b' models', b' do', b' not', b' use', b' a', b' fixed', b' word', b' vocabulary', b',', b' but', b' rather', b' a', b' token', b' vocabulary', b' (', b'based', b' on', b' sub', b'words', b' using', b' B', b'PE', b' or', b' Word', b'Piece', b').']\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"These models do not use a fixed word vocabulary, but rather a token vocabulary (based on subwords using BPE or WordPiece).\"\n",
    "example_tokenization(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length (bytes): 128 \n",
      "\n",
      "BERT tokenization:\n",
      "Tokens: [101, 1037, 19939, 2075, 2971, 2005, 1996, 3935, 3698, 4083, 2607, 1010, 2029, 2003, 2112, 1997, 1996, 1049, 1012, 8040, 1012, 3014, 2012, 14365, 2386, 2118, 102]\n",
      "Words: ['[CLS]', 'a', 'forecast', '##ing', 'competition', 'for', 'the', 'advanced', 'machine', 'learning', 'course', ',', 'which', 'is', 'part', 'of', 'the', 'm', '.', 'sc', '.', 'degree', 'at', 'reich', '##man', 'university', '[SEP]']\n",
      "27 tokens, about 54 bytes\n",
      "\n",
      "FastText tokenization:\n",
      "Tokens (words): ['A', 'Forecasting', 'competition', 'for', 'the', 'Advanced', 'Machine', 'Learning', 'course,', 'which', 'is', 'part', 'of', 'the', 'M.Sc.', 'degree', 'at', 'Reichman', 'University']\n",
      "19\n",
      "\n",
      "OpenAI tokenization:\n",
      "Tokens: [262, 362, 56775, 287, 10937, 369, 279, 21844, 13257, 21579, 3388, 11, 902, 374, 961, 315, 279, 386, 18832, 13, 8547, 520, 51659, 1543, 3907]\n",
      "Words: [b'   ', b' A', b' Forecast', b'ing', b' competition', b' for', b' the', b' Advanced', b' Machine', b' Learning', b' course', b',', b' which', b' is', b' part', b' of', b' the', b' M', b'.Sc', b'.', b' degree', b' at', b' Reich', b'man', b' University']\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"A Forecasting competition for the Advanced Machine Learning course, which is part of the M.Sc. degree at Reichman University\"\n",
    "example_tokenization(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average word length: 5.58 characters\n"
     ]
    }
   ],
   "source": [
    "# Calculate average word length\n",
    "text = \"A Forecasting competition for the Advanced Machine Learning course, which is part of the M.Sc. degree at Reichman University\"\n",
    "words = text.split()\n",
    "avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "print(f\"\\nAverage word length: {avg_word_length:.2f} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length (bytes): 1778\n",
      "\n",
      "Average word length: 5.63 characters ) add one for space\n",
      "\n",
      "BERT tokenization:\n",
      "Tokens: [101, 13063, 1512, 117, 17881, 1527, 131, 20100, 1144, 2536, 1157, 1148, 1518, 5294, 1114, 1126, 1890, 4633, 3107, 117, 8362, 2707, 10689, 170, 2079, 1104, 4844, 1174, 2865, 14719, 3768, 1118, 1103, 1583, 787, 188, 2890, 5069, 118, 5694, 4884, 119, 25423, 3338, 6175, 118, 1359, 4097, 16162, 2064, 1742, 2158, 16162, 2924, 17481, 1162, 117, 1103, 15302, 2555, 14719, 1105, 12647, 24971, 2436, 789, 20384, 1535, 1154, 4799, 790, 1229, 14118, 789, 1890, 2754, 1105, 22009, 15924, 117, 790, 2452, 1106, 170, 20100, 3181, 1836, 119, 1109, 10915, 787, 1207, 2322, 117, 11770, 1142, 1989, 117, 1956, 1890, 9469, 1116, 27901, 3080, 26363, 11945, 17305, 10589, 1105, 156, 2328, 8057, 2646, 159, 1200, 1918, 117, 3338, 11113, 1760, 16138, 15147, 1105, 24360, 153, 16383, 20165, 119, 138, 14566, 8223, 18266, 1158, 4351, 117, 2046, 1118, 5719, 4633, 8152, 139, 10131, 1204, 14159, 19610, 117, 1437, 1103, 2130, 7653, 23614, 1107, 1103, 3432, 1331, 1104, 19183, 4093, 783, 1259, 1113, 1157, 15565, 2585, 3192, 1116, 119, 1109, 1815, 7981, 20100, 787, 188, 8978, 21681, 1111, 170, 2319, 1107, 1134, 1122, 1144, 4531, 3216, 20733, 1290, 5273, 117, 2258, 170, 15469, 2239, 117, 1107, 1876, 119, 1258, 1628, 170, 12907, 2205, 7049, 2551, 1201, 1224, 117, 1103, 1419, 176, 16033, 1181, 3777, 1113, 1103, 1583, 787, 188, 1211, 1927, 4799, 117, 5428, 117, 5405, 9521, 24930, 23358, 1105, 11336, 15581, 5926, 1106, 170, 122, 119, 5311, 118, 3775, 118, 187, 26939, 1162, 113, 1173, 109, 3140, 1550, 114, 2239, 1106, 11378, 1103, 1890, 1569, 1264, 1107, 1478, 119, 1252, 20100, 1144, 1290, 6325, 1106, 1294, 2595, 1107, 16038, 117, 1114, 1469, 2394, 7516, 1107, 10351, 1115, 1122, 1125, 26551, 1103, 1295, 1104, 4822, 1107, 1726, 1106, 4214, 117, 1205, 1121, 170, 4709, 1104, 8301, 119, 1109, 4097, 787, 188, 3265, 22605, 1971, 6802, 1198, 5429, 4822, 1107, 1726, 117, 3402, 1106, 1167, 1190, 123, 117, 4372, 1107, 8684, 1975, 117, 170, 2319, 1104, 12763, 1416, 119, 1109, 4799, 14719, 4994, 1145, 2691, 1106, 1129, 5859, 1157, 1535, 787, 188, 12647, 24971, 1671, 117, 1134, 1144, 7440, 2495, 12165, 1481, 1157, 1441, 1116, 14719, 119, 3728, 2793, 7827, 1138, 24787, 1174, 2130, 7653, 117, 1114, 1142, 1214, 787, 188, 3198, 5308, 8050, 3022, 160, 2249, 8215, 2851, 140, 21263, 2836, 4425, 1105, 3557, 24360, 156, 2328, 787, 12639, 1182, 9186, 119, 102]\n",
      "Words: ['[CLS]', 'Dec', '25', ',', '202', '##4', ':', 'Nike', 'has', 'launched', 'its', 'first', 'ever', 'collaboration', 'with', 'an', 'Indian', 'fashion', 'label', ',', 'un', '##ve', '##iling', 'a', 'range', 'of', 'pattern', '##ed', 'sports', '##wear', 'inspired', 'by', 'the', 'country', '’', 's', 'ancient', 'tie', '-', 'dying', 'techniques', '.', 'Created', 'alongside', 'Delhi', '-', 'based', 'brand', 'Nor', '##B', '##la', '##ck', 'Nor', '##W', '##hit', '##e', ',', 'the', 'colorful', 'foot', '##wear', 'and', 'app', '##arel', 'collection', '“', 'invites', 'women', 'into', 'sport', '”', 'while', 'celebrating', '“', 'Indian', 'culture', 'and', 'crafts', '##manship', ',', '”', 'according', 'to', 'a', 'Nike', 'press', 'release', '.', 'The', 'brands', '’', 'new', 'campaign', ',', 'unveiled', 'this', 'week', ',', 'features', 'Indian', 'cricketer', '##s', 'Je', '##mi', '##mah', 'Rod', '##rig', '##ues', 'and', 'S', '##ha', '##fa', '##li', 'V', '##er', '##ma', ',', 'alongside', 'wrestler', 'An', '##shu', 'Malik', 'and', 'sprinter', 'P', '##riya', 'Mohan', '.', 'A', '##cco', '##mp', '##any', '##ing', 'images', ',', 'shot', 'by', 'celebrated', 'fashion', 'photographer', 'B', '##hara', '##t', 'Si', '##kka', ',', 'show', 'the', 'female', 'athletes', 'posing', 'in', 'the', 'historic', 'city', 'of', 'Jai', '##pur', '—', 'including', 'on', 'its', 'iconic', 'step', '##well', '##s', '.', 'The', 'move', 'signals', 'Nike', '’', 's', 'renewed', 'ambitions', 'for', 'a', 'market', 'in', 'which', 'it', 'has', 'experienced', 'mixed', 'fortunes', 'since', 'entering', ',', 'via', 'a', 'licensing', 'deal', ',', 'in', '1995', '.', 'After', 'established', 'a', 'wholly', 'owned', 'subsidiary', 'nine', 'years', 'later', ',', 'the', 'company', 'g', '##amble', '##d', 'heavily', 'on', 'the', 'country', '’', 's', 'most', 'popular', 'sport', ',', 'cricket', ',', 'beating', 'rivals', 'Ad', '##idas', 'and', 'Re', '##eb', '##ok', 'to', 'a', '1', '.', '97', '-', 'billion', '-', 'r', '##upe', '##e', '(', 'then', '$', '44', 'million', ')', 'deal', 'to', 'outfit', 'the', 'Indian', 'national', 'team', 'in', '2005', '.', 'But', 'Nike', 'has', 'since', 'struggled', 'to', 'make', 'commercial', 'in', '##roads', ',', 'with', 'local', 'media', 'reporting', 'in', '2019', 'that', 'it', 'had', 'slashed', 'the', 'number', 'of', 'stores', 'in', 'India', 'to', '150', ',', 'down', 'from', 'a', 'peak', 'of', '350', '.', 'The', 'brand', '’', 's', 'website', 'directory', 'currently', 'lists', 'just', '93', 'stores', 'in', 'India', ',', 'compared', 'to', 'more', 'than', '2', ',', '600', 'in', 'mainland', 'China', ',', 'a', 'market', 'of', 'comparable', 'population', '.', 'The', 'sport', '##wear', 'giant', 'also', 'appears', 'to', 'be', 'pushing', 'its', 'women', '’', 's', 'app', '##arel', 'business', ',', 'which', 'has', 'traditionally', 'la', '##gged', 'behind', 'its', 'men', '##s', '##wear', '.', 'Several', 'recent', 'campaigns', 'have', 'spotlight', '##ed', 'female', 'athletes', ',', 'with', 'this', 'year', '’', 's', 'Super', 'Bowl', 'ad', 'featuring', 'W', '##N', '##BA', 'star', 'C', '##ait', '##lin', 'Clark', 'and', 'Olympic', 'sprinter', 'S', '##ha', '’', 'Carr', '##i', 'Richardson', '.', '[SEP]']\n",
      "391 tokens, about 782 bytes\n",
      "\n",
      "OpenAI tokenization:\n",
      "Tokens: [198, 5005, 220, 914, 11, 220, 2366, 19, 25, 34244, 706, 11887, 1202, 1176, 3596, 20632, 449, 459, 7904, 11401, 2440, 11, 92588, 264, 2134, 315, 5497, 291, 10775, 2332, 686, 14948, 555, 279, 3224, 753, 14154, 18623, 1773, 7169, 12823, 627, 11956, 16662, 22767, 6108, 6883, 8170, 14755, 8170, 14404, 11, 279, 34966, 68172, 323, 55425, 4526, 1054, 14386, 3695, 3278, 1139, 10775, 863, 1418, 32689, 1054, 48664, 7829, 323, 85060, 2476, 4184, 311, 264, 34244, 3577, 4984, 627, 791, 16097, 529, 502, 4901, 11, 39297, 420, 2046, 11, 4519, 7904, 1589, 875, 2481, 622, 336, 318, 1494, 37485, 1157, 323, 1443, 2642, 8115, 6383, 1764, 11, 16662, 83162, 1556, 939, 84, 73320, 323, 38949, 261, 28885, 7911, 20409, 276, 13, 54064, 2023, 287, 5448, 11, 6689, 555, 28284, 11401, 29867, 67692, 266, 78449, 4657, 11, 1501, 279, 8954, 23579, 53004, 304, 279, 18526, 3363, 315, 23720, 72977, 2001, 2737, 389, 1202, 27373, 3094, 86, 6572, 627, 791, 3351, 17738, 34244, 753, 36646, 51566, 369, 264, 3157, 304, 902, 433, 706, 10534, 9709, 68603, 2533, 16661, 11, 4669, 264, 28506, 3568, 11, 304, 220, 2550, 20, 13, 4740, 9749, 264, 42241, 13234, 41164, 11888, 1667, 3010, 11, 279, 2883, 76034, 839, 17345, 389, 279, 3224, 753, 1455, 5526, 10775, 11, 37099, 11, 27242, 35938, 70989, 323, 1050, 3141, 564, 311, 264, 220, 16, 13, 3534, 70173, 12, 2739, 62598, 320, 3473, 400, 2096, 3610, 8, 3568, 311, 28403, 279, 7904, 5426, 2128, 304, 220, 1049, 20, 627, 4071, 34244, 706, 2533, 28214, 311, 1304, 8518, 304, 43791, 11, 449, 2254, 3772, 13122, 304, 220, 679, 24, 430, 433, 1047, 84624, 279, 1396, 315, 10756, 304, 6890, 311, 220, 3965, 11, 1523, 505, 264, 16557, 315, 220, 8652, 13, 578, 6883, 753, 3997, 6352, 5131, 11725, 1120, 220, 6365, 10756, 304, 6890, 11, 7863, 311, 810, 1109, 220, 17, 11, 5067, 304, 51115, 5734, 11, 264, 3157, 315, 30139, 7187, 627, 791, 10775, 23581, 14880, 1101, 8111, 311, 387, 17919, 1202, 3278, 753, 55425, 2626, 11, 902, 706, 36342, 22171, 3640, 4920, 1202, 16434, 23581, 13, 26778, 3293, 21343, 617, 37973, 291, 8954, 23579, 11, 449, 420, 1060, 753, 7445, 20904, 1008, 16850, 78364, 7209, 6917, 76432, 3817, 22010, 323, 25944, 38949, 261, 29070, 529, 9028, 462, 46823, 627]\n",
      "Words: [b'\\n', b'Dec', b' ', b'25', b',', b' ', b'202', b'4', b':', b' Nike', b' has', b' launched', b' its', b' first', b' ever', b' collaboration', b' with', b' an', b' Indian', b' fashion', b' label', b',', b' unveiling', b' a', b' range', b' of', b' pattern', b'ed', b' sport', b'sw', b'ear', b' inspired', b' by', b' the', b' country', b'\\xe2\\x80\\x99s', b' ancient', b' tie', b'-d', b'ying', b' techniques', b'.\\n', b'Created', b' alongside', b' Delhi', b'-based', b' brand', b' Nor', b'Black', b' Nor', b'White', b',', b' the', b' colorful', b' footwear', b' and', b' apparel', b' collection', b' \\xe2\\x80\\x9c', b'inv', b'ites', b' women', b' into', b' sport', b'\\xe2\\x80\\x9d', b' while', b' celebrating', b' \\xe2\\x80\\x9c', b'Indian', b' culture', b' and', b' craftsmanship', b',\\xe2\\x80\\x9d', b' according', b' to', b' a', b' Nike', b' press', b' release', b'.\\n', b'The', b' brands', b'\\xe2\\x80\\x99', b' new', b' campaign', b',', b' unveiled', b' this', b' week', b',', b' features', b' Indian', b' cr', b'ick', b'eters', b' J', b'em', b'im', b'ah', b' Rodrig', b'ues', b' and', b' Sh', b'af', b'ali', b' Ver', b'ma', b',', b' alongside', b' wrestler', b' An', b'sh', b'u', b' Malik', b' and', b' sprint', b'er', b' Pri', b'ya', b' Moh', b'an', b'.', b' Accom', b'pany', b'ing', b' images', b',', b' shot', b' by', b' celebrated', b' fashion', b' photographer', b' Bhar', b'at', b' Sik', b'ka', b',', b' show', b' the', b' female', b' athletes', b' posing', b' in', b' the', b' historic', b' city', b' of', b' Ja', b'ipur', b' \\xe2\\x80\\x94', b' including', b' on', b' its', b' iconic', b' step', b'w', b'ells', b'.\\n', b'The', b' move', b' signals', b' Nike', b'\\xe2\\x80\\x99s', b' renewed', b' ambitions', b' for', b' a', b' market', b' in', b' which', b' it', b' has', b' experienced', b' mixed', b' fortunes', b' since', b' entering', b',', b' via', b' a', b' licensing', b' deal', b',', b' in', b' ', b'199', b'5', b'.', b' After', b' established', b' a', b' wholly', b' owned', b' subsidiary', b' nine', b' years', b' later', b',', b' the', b' company', b' gamb', b'led', b' heavily', b' on', b' the', b' country', b'\\xe2\\x80\\x99s', b' most', b' popular', b' sport', b',', b' cricket', b',', b' beating', b' rivals', b' Adidas', b' and', b' Re', b'eb', b'ok', b' to', b' a', b' ', b'1', b'.', b'97', b'-billion', b'-', b'ru', b'pee', b' (', b'then', b' $', b'44', b' million', b')', b' deal', b' to', b' outfit', b' the', b' Indian', b' national', b' team', b' in', b' ', b'200', b'5', b'.\\n', b'But', b' Nike', b' has', b' since', b' struggled', b' to', b' make', b' commercial', b' in', b'roads', b',', b' with', b' local', b' media', b' reporting', b' in', b' ', b'201', b'9', b' that', b' it', b' had', b' slashed', b' the', b' number', b' of', b' stores', b' in', b' India', b' to', b' ', b'150', b',', b' down', b' from', b' a', b' peak', b' of', b' ', b'350', b'.', b' The', b' brand', b'\\xe2\\x80\\x99s', b' website', b' directory', b' currently', b' lists', b' just', b' ', b'93', b' stores', b' in', b' India', b',', b' compared', b' to', b' more', b' than', b' ', b'2', b',', b'600', b' in', b' mainland', b' China', b',', b' a', b' market', b' of', b' comparable', b' population', b'.\\n', b'The', b' sport', b'wear', b' giant', b' also', b' appears', b' to', b' be', b' pushing', b' its', b' women', b'\\xe2\\x80\\x99s', b' apparel', b' business', b',', b' which', b' has', b' traditionally', b' lag', b'ged', b' behind', b' its', b' mens', b'wear', b'.', b' Several', b' recent', b' campaigns', b' have', b' spotlight', b'ed', b' female', b' athletes', b',', b' with', b' this', b' year', b'\\xe2\\x80\\x99s', b' Super', b' Bowl', b' ad', b' featuring', b' WN', b'BA', b' star', b' Cait', b'lin', b' Clark', b' and', b' Olympic', b' sprint', b'er', b' Sha', b'\\xe2\\x80\\x99', b'Car', b'ri', b' Richardson', b'.\\n']\n",
      "379 tokens, about 1137 bytes\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Dec 25, 2024: Nike has launched its first ever collaboration with an Indian fashion label, unveiling a range of patterned sportswear inspired by the country’s ancient tie-dying techniques.\n",
    "Created alongside Delhi-based brand NorBlack NorWhite, the colorful footwear and apparel collection “invites women into sport” while celebrating “Indian culture and craftsmanship,” according to a Nike press release.\n",
    "The brands’ new campaign, unveiled this week, features Indian cricketers Jemimah Rodrigues and Shafali Verma, alongside wrestler Anshu Malik and sprinter Priya Mohan. Accompanying images, shot by celebrated fashion photographer Bharat Sikka, show the female athletes posing in the historic city of Jaipur — including on its iconic stepwells.\n",
    "The move signals Nike’s renewed ambitions for a market in which it has experienced mixed fortunes since entering, via a licensing deal, in 1995. After established a wholly owned subsidiary nine years later, the company gambled heavily on the country’s most popular sport, cricket, beating rivals Adidas and Reebok to a 1.97-billion-rupee (then $44 million) deal to outfit the Indian national team in 2005.\n",
    "But Nike has since struggled to make commercial inroads, with local media reporting in 2019 that it had slashed the number of stores in India to 150, down from a peak of 350. The brand’s website directory currently lists just 93 stores in India, compared to more than 2,600 in mainland China, a market of comparable population.\n",
    "The sportwear giant also appears to be pushing its women’s apparel business, which has traditionally lagged behind its menswear. Several recent campaigns have spotlighted female athletes, with this year’s Super Bowl ad featuring WNBA star Caitlin Clark and Olympic sprinter Sha’Carri Richardson.\n",
    "\"\"\"\n",
    "bert_words, openai_words = example_tokenization(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDec 25, 2024: Nike has launched its first ever collaboration with an Indian fashion label, unveiling a range of patterned sportswear inspired by the country’s ancient tie-dying techniques.\\nCreated alongside Delhi-based brand NorBlack NorWhite, the colorful footwear and apparel collection “invites women into sport” while celebrating “Indian culture and craftsmanship,” according to a Nike press release.\\nThe brands’ new campaign, unveiled this week, features Indian cricketers Jemimah Rodrigues and Shafali Verma, alongside wrestler Anshu Malik and sprinter Priya Mohan. Accompanying images, shot by celebrated fashion photographer Bharat Sikka, show the female athletes posing in the historic city of Jaipur — including on its iconic stepwells.\\nThe move signals Nike’s renewed ambitions for a market in which it has experienced mixed fortunes since entering, via a licensing deal, in 1995. After established a wholly owned subsidiary nine years later, the company gambled heavily on the country’s most popular sport, cricket, beating rivals Adidas and Reebok to a 1.97-billion-rupee (then $44 million) deal to outfit the Indian national team in 2005.\\nBut Nike has since struggled to make commercial inroads, with local media reporting in 2019 that it had slashed the number of stores in India to 150, down from a peak of 350. The brand’s website directory currently lists just 93 stores in India, compared to more than 2,600 in mainland China, a market of comparable population.\\nThe sportwear giant also appears to be pushing its women’s apparel business, which has traditionally lagged behind its menswear. Several recent campaigns have spotlighted female athletes, with this year’s Super Bowl ad featuring WNBA star Caitlin Clark and Olympic sprinter Sha’Carri Richardson.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert bytes to strings in openai_words list\n",
    "openai_words = [word.decode('utf-8') if isinstance(word, bytes) else word for word in openai_words]\n",
    "openai_words_string = ''.join(openai_words)\n",
    "openai_words_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text == openai_words_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text equals BERT reconstructed text: False\n"
     ]
    }
   ],
   "source": [
    "# Join BERT words, handling special '##' prefix\n",
    "bert_text = \"\"\n",
    "for i, word in enumerate(bert_words):\n",
    "    if word == '[CLS]':\n",
    "        continue\n",
    "    if word == '[SEP]':\n",
    "        continue\n",
    "    if isinstance(word, bytes):\n",
    "        word = word.decode('utf-8')\n",
    "    if word.startswith('##'):\n",
    "        bert_text += word[2:]  # Remove '##' and append directly\n",
    "    else:\n",
    "        if i > 0:  # Add space before word unless it's the first word\n",
    "            bert_text += \" \"\n",
    "        bert_text += word\n",
    "\n",
    "print(f\"\\nOriginal text equals BERT reconstructed text: {text == bert_text}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Dec25,2024:NikehaslauncheditsfirstevercollaborationwithanIndianfashionlabel,unveilingarangeofpatternedsportswearinspiredbythecountry’sancienttie-dyingtechniques.CreatedalongsideDelhi-basedbrandNorBlackNorWhite,thecolorfulfootwearandapparelcollection“inviteswomenintosport”whilecelebrating“Indiancultureandcraftsmanship,”accordingtoaNikepressrelease.Thebrands’newcampaign,unveiledthisweek,featuresIndiancricketersJemimahRodriguesandShafaliVerma,alongsidewrestlerAnshuMalikandsprinterPriyaMohan.Accompanyingimages,shotbycelebratedfashionphotographerBharatSikka,showthefemaleathletesposinginthehistoriccityofJaipur—includingonitsiconicstepwells.ThemovesignalsNike’srenewedambitionsforamarketinwhichithasexperiencedmixedfortunessinceentering,viaalicensingdeal,in1995.Afterestablishedawhollyownedsubsidiarynineyearslater,thecompanygambledheavilyonthecountry’smostpopularsport,cricket,beatingrivalsAdidasandReeboktoa1.97-billion-rupee(then$44million)dealtooutfittheIndiannationalteamin2005.ButNikehassincestruggledtomakecommercialinroads,withlocalmediareportingin2019thatithadslashedthenumberofstoresinIndiato150,downfromapeakof350.Thebrand’swebsitedirectorycurrentlylistsjust93storesinIndia,comparedtomorethan2,600inmainlandChina,amarketofcomparablepopulation.Thesportweargiantalsoappearstobepushingitswomen’sapparelbusiness,whichhastraditionallylaggedbehinditsmenswear.Severalrecentcampaignshavespotlightedfemaleathletes,withthisyear’sSuperBowladfeaturingWNBAstarCaitlinClarkandOlympicsprinterSha’CarriRichardson.\n",
      "Dec25,2024:NikehaslauncheditsfirstevercollaborationwithanIndianfashionlabel,unveilingarangeofpatternedsportswearinspiredbythecountry’sancienttie-dyingtechniques.CreatedalongsideDelhi-basedbrandNorBlackNorWhite,thecolorfulfootwearandapparelcollection“inviteswomenintosport”whilecelebrating“Indiancultureandcraftsmanship,”accordingtoaNikepressrelease.Thebrands’newcampaign,unveiledthisweek,featuresIndiancricketersJemimahRodriguesandShafaliVerma,alongsidewrestlerAnshuMalikandsprinterPriyaMohan.Accompanyingimages,shotbycelebratedfashionphotographerBharatSikka,showthefemaleathletesposinginthehistoriccityofJaipur—includingonitsiconicstepwells.ThemovesignalsNike’srenewedambitionsforamarketinwhichithasexperiencedmixedfortunessinceentering,viaalicensingdeal,in1995.Afterestablishedawhollyownedsubsidiarynineyearslater,thecompanygambledheavilyonthecountry’smostpopularsport,cricket,beatingrivalsAdidasandReeboktoa1.97-billion-rupee(then$44million)dealtooutfittheIndiannationalteamin2005.ButNikehassincestruggledtomakecommercialinroads,withlocalmediareportingin2019thatithadslashedthenumberofstoresinIndiato150,downfromapeakof350.Thebrand’swebsitedirectorycurrentlylistsjust93storesinIndia,comparedtomorethan2,600inmainlandChina,amarketofcomparablepopulation.Thesportweargiantalsoappearstobepushingitswomen’sapparelbusiness,whichhastraditionallylaggedbehinditsmenswear.Severalrecentcampaignshavespotlightedfemaleathletes,withthisyear’sSuperBowladfeaturingWNBAstarCaitlinClarkandOlympicsprinterSha’CarriRichardson.\n"
     ]
    }
   ],
   "source": [
    "stripped_text = text.replace(' ', '').replace('\\n', '')\n",
    "stripped_bert_text = bert_text.replace(' ', '').replace('\\n', '')\n",
    "print(stripped_text == stripped_bert_text)\n",
    "print(stripped_text)\n",
    "print(stripped_bert_text)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13063: Dec\n",
      "13064: ##aneous\n",
      "13065: chambers\n",
      "13066: Color\n",
      "13067: Gus\n",
      "13068: ##site\n",
      "13069: Alternative\n",
      "13070: ##world\n",
      "13071: Exeter\n",
      "13072: Omaha\n",
      "13073: celebrities\n",
      "13074: striker\n",
      "13075: 210\n",
      "13076: dwarf\n",
      "13077: meals\n",
      "13078: Oriental\n",
      "13079: Pearson\n",
      "13080: financing\n",
      "13081: revenues\n",
      "13082: underwater\n",
      "13083: Steele\n",
      "13084: screw\n",
      "13085: Feeling\n",
      "13086: Mt\n",
      "13087: acids\n",
      "13088: badge\n",
      "13089: swore\n",
      "13090: theaters\n",
      "13091: Moving\n",
      "13092: admired\n",
      "13093: lung\n",
      "13094: knot\n",
      "13095: penalties\n",
      "13096: 116\n",
      "13097: fork\n",
      "13098: ##cribed\n",
      "13099: Afghan\n",
      "13100: outskirts\n",
      "13101: Cambodia\n",
      "13102: oval\n",
      "13103: wool\n",
      "13104: fossils\n",
      "13105: Ned\n",
      "13106: Countess\n",
      "13107: Darkness\n",
      "13108: delicious\n",
      "13109: ##nica\n",
      "13110: Evelyn\n",
      "13111: Recordings\n",
      "13112: guidelines\n",
      "13113: ##CP\n",
      "13114: Sandra\n",
      "13115: meantime\n",
      "13116: Antarctica\n",
      "13117: modeling\n",
      "13118: granddaughter\n",
      "13119: ##rial\n",
      "13120: Roma\n",
      "13121: Seventh\n",
      "13122: Sunshine\n",
      "13123: Gabe\n",
      "13124: ##nton\n",
      "13125: Shop\n",
      "13126: Turks\n",
      "13127: prolific\n",
      "13128: soup\n",
      "13129: parody\n",
      "13130: ##nta\n",
      "13131: Judith\n",
      "13132: disciplines\n",
      "13133: resign\n",
      "13134: Companies\n",
      "13135: Libya\n",
      "13136: Jets\n",
      "13137: inserted\n",
      "13138: Mile\n",
      "13139: retrieve\n",
      "13140: filmmaker\n",
      "13141: ##rand\n",
      "13142: realistic\n",
      "13143: unhappy\n",
      "13144: ##30\n",
      "13145: sandstone\n",
      "13146: ##nas\n",
      "13147: ##lent\n",
      "13148: ##ush\n",
      "13149: ##rous\n",
      "13150: Brent\n",
      "13151: trash\n",
      "13152: Rescue\n",
      "13153: ##unted\n",
      "13154: Autumn\n",
      "13155: disgust\n",
      "13156: flexible\n",
      "13157: infinite\n",
      "13158: sideways\n",
      "13159: ##oss\n",
      "13160: ##vik\n",
      "13161: trailing\n",
      "13162: disturbed\n",
      "13163: 50th\n",
      "13164: Newark\n",
      "13165: posthumously\n",
      "13166: ##rol\n",
      "13167: Schmidt\n",
      "13168: Josef\n",
      "13169: ##eous\n",
      "13170: determining\n",
      "13171: menu\n",
      "13172: Pole\n",
      "13173: Anita\n",
      "13174: Luc\n",
      "13175: peaks\n",
      "13176: 118\n",
      "13177: Yard\n",
      "13178: warrant\n",
      "13179: generic\n",
      "13180: deserted\n",
      "13181: Walking\n",
      "13182: stamp\n",
      "13183: tracked\n",
      "13184: ##berger\n",
      "13185: paired\n",
      "13186: surveyed\n",
      "13187: sued\n",
      "13188: Rainbow\n",
      "13189: ##isk\n",
      "13190: Carpenter\n",
      "13191: submarines\n",
      "13192: realization\n",
      "13193: touches\n",
      "13194: sweeping\n",
      "13195: Fritz\n",
      "13196: module\n",
      "13197: Whether\n",
      "13198: resembles\n",
      "13199: ##form\n",
      "13200: ##lop\n",
      "13201: unsure\n",
      "13202: hunters\n",
      "13203: Zagreb\n",
      "13204: unemployment\n",
      "13205: Senators\n",
      "13206: Georgetown\n",
      "13207: ##onic\n",
      "13208: Barker\n",
      "13209: foul\n",
      "13210: commercials\n",
      "13211: Dresden\n",
      "13212: Words\n",
      "13213: collision\n",
      "13214: Carlton\n",
      "13215: Fashion\n",
      "13216: doubted\n",
      "13217: ##ril\n",
      "13218: precision\n",
      "13219: MIT\n",
      "13220: Jacobs\n",
      "13221: mob\n",
      "13222: Monk\n",
      "13223: retaining\n",
      "13224: gotta\n",
      "13225: ##rod\n",
      "13226: remake\n",
      "13227: Fast\n",
      "13228: chips\n",
      "13229: ##pled\n",
      "13230: sufficiently\n",
      "13231: ##lights\n",
      "13232: delivering\n",
      "13233: ##enburg\n",
      "13234: Dancing\n",
      "13235: Barton\n",
      "13236: Officers\n",
      "13237: metals\n",
      "13238: ##lake\n",
      "13239: religions\n",
      "13240: ##ré\n",
      "13241: motivated\n",
      "13242: differs\n",
      "13243: dorsal\n",
      "13244: ##birds\n",
      "13245: ##rts\n",
      "13246: Priest\n",
      "13247: polished\n",
      "13248: ##aling\n",
      "13249: Saxony\n",
      "13250: Wyatt\n",
      "13251: knockout\n",
      "13252: ##hor\n",
      "13253: Lopez\n",
      "13254: RNA\n",
      "13255: ##link\n",
      "13256: metallic\n",
      "13257: ##kas\n",
      "13258: daylight\n",
      "13259: Montenegro\n",
      "13260: ##lining\n",
      "13261: wrapping\n",
      "13262: resemble\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "vocab_items = list(tokenizer.vocab.items())\n",
    "first_200 = vocab_items[13063 : 13063 + 200]\n",
    "\n",
    "# Print each token and its ID\n",
    "for token, token_id in first_200:\n",
    "    print(f\"{token_id}: {token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\Me\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Me\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load BERT model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertModel\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Get the token ID for the word \"word\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Me\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2045\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2045\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2046\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Me\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2075\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\Me\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2073\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2072\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2073\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\Me\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32mc:\\Users\\Me\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:44\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_attn_mask_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     34\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     35\u001b[0m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     TokenClassifierOutput,\n\u001b[0;32m     43\u001b[0m )\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelOutput, auto_docstring, get_torch_version, logging\n",
      "File \u001b[1;32mc:\\Users\\Me\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:73\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdpa_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sdpa_attention_forward\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_parallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     65\u001b[0m     ALL_PARALLEL_STYLES,\n\u001b[0;32m     66\u001b[0m     _get_parameter_tp_plan,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m     verify_tp_plan,\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     Conv1D,\n\u001b[0;32m     76\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m     prune_linear_layer,\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoHfQuantizer, HfQuantizer\n",
      "File \u001b[1;32mc:\\Users\\Me\\anaconda3\\Lib\\site-packages\\transformers\\loss\\loss_utils.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_d_fine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DFineForObjectDetectionLoss\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_deformable_detr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n",
      "File \u001b[1;32mc:\\Users\\Me\\anaconda3\\Lib\\site-packages\\transformers\\loss\\loss_d_fine.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_vision_available\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     box_iou,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_rt_detr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RTDetrHungarianMatcher, RTDetrLoss\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n",
      "File \u001b[1;32mc:\\Users\\Me\\anaconda3\\Lib\\site-packages\\transformers\\loss\\loss_for_object_detection.py:32\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_sum_assignment\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_to_corners_format\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdice_loss\u001b[39m(inputs, targets, num_boxes):\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    Compute the DICE loss, similar to generalized IOU for masks\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m                 class).\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Me\\anaconda3\\Lib\\site-packages\\transformers\\image_transforms.py:47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Me\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\Me\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\Me\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "# Load BERT model\n",
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Get the token ID for the word \"word\"\n",
    "word_token = tokenizer.encode(\"word\", add_special_tokens=False)[0]\n",
    "print(f\"\\nToken ID for 'word': {word_token}\")\n",
    "\n",
    "# Get the embedding for this token from the model's embedding layer\n",
    "word_embedding = model.embeddings.word_embeddings(torch.tensor([word_token]))\n",
    "print(f\"\\nEmbedding vector for 'word' (shape {word_embedding.shape}):\")\n",
    "print(word_embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lamplezip\n",
    "wordpiece\n",
    "\n",
    "positive vs Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
